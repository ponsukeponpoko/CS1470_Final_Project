x: (20425, 32)
y: (20425,)

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 32, 100)           12600     
                                                                 
 conv1d (Conv1D)             (None, 32, 64)            19264     
                                                                 
 dropout (Dropout)           (None, 32, 64)            0         
                                                                 
 max_pooling1d (MaxPooling1D  (None, 16, 64)           0         
 )                                                               
                                                                 
 conv1d_1 (Conv1D)           (None, 16, 128)           24704     
                                                                 
 dropout_1 (Dropout)         (None, 16, 128)           0         
                                                                 
 max_pooling1d_1 (MaxPooling  (None, 8, 128)           0         
 1D)                                                             
                                                                 
 conv1d_2 (Conv1D)           (None, 8, 256)            98560     
                                                                 
 dropout_2 (Dropout)         (None, 8, 256)            0         
                                                                 
 max_pooling1d_2 (MaxPooling  (None, 4, 256)           0         
 1D)                                                             
                                                                 
 conv1d_3 (Conv1D)           (None, 4, 256)            196864    
                                                                 
 dropout_3 (Dropout)         (None, 4, 256)            0         
                                                                 
 max_pooling1d_3 (MaxPooling  (None, 2, 256)           0         
 1D)                                                             
                                                                 
 global_max_pooling1d (Globa  (None, 256)              0         
 lMaxPooling1D)                                                  
                                                                 
 dense (Dense)               (None, 256)               65792     
                                                                 
 dense_1 (Dense)             (None, 126)               32382     
                                                                 
=================================================================
Total params: 450,166
Trainable params: 450,166
Non-trainable params: 0
_________________________________________________________________
unique x: 126
unique y: 126
Epoch 1/250
2022-05-11 06:07:52.887784: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8200
128/128 [==============================] - ETA: 0s - loss: 4.5647  
Epoch 1: val_loss improved from inf to 4.60109, saving model to best_model-wavenet.h5
128/128 [==============================] - 5s 18ms/step - loss: 4.5647 - val_loss: 4.6011
Epoch 2/250
126/128 [============================>.] - ETA: 0s - loss: 4.4774
Epoch 2: val_loss improved from 4.60109 to 4.45301, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 4.4770 - val_loss: 4.4530
Epoch 3/250
126/128 [============================>.] - ETA: 0s - loss: 4.3601
Epoch 3: val_loss improved from 4.45301 to 4.37542, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 4.3598 - val_loss: 4.3754
Epoch 4/250
126/128 [============================>.] - ETA: 0s - loss: 4.2795
Epoch 4: val_loss improved from 4.37542 to 4.34819, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 4.2806 - val_loss: 4.3482
Epoch 5/250
125/128 [============================>.] - ETA: 0s - loss: 4.2132
Epoch 5: val_loss improved from 4.34819 to 4.30310, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 15ms/step - loss: 4.2123 - val_loss: 4.3031
Epoch 6/250
126/128 [============================>.] - ETA: 0s - loss: 4.1612
Epoch 6: val_loss improved from 4.30310 to 4.27080, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 4.1604 - val_loss: 4.2708
Epoch 7/250
126/128 [============================>.] - ETA: 0s - loss: 4.1150
Epoch 7: val_loss improved from 4.27080 to 4.23098, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 4.1151 - val_loss: 4.2310
Epoch 8/250
126/128 [============================>.] - ETA: 0s - loss: 4.0644
Epoch 8: val_loss improved from 4.23098 to 4.20152, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 4.0638 - val_loss: 4.2015
Epoch 9/250
126/128 [============================>.] - ETA: 0s - loss: 4.0155
Epoch 9: val_loss improved from 4.20152 to 4.15253, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 4.0166 - val_loss: 4.1525
Epoch 10/250
125/128 [============================>.] - ETA: 0s - loss: 3.9693
Epoch 10: val_loss improved from 4.15253 to 4.13244, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.9701 - val_loss: 4.1324
Epoch 11/250
125/128 [============================>.] - ETA: 0s - loss: 3.9271
Epoch 11: val_loss improved from 4.13244 to 4.08587, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.9256 - val_loss: 4.0859
Epoch 12/250
126/128 [============================>.] - ETA: 0s - loss: 3.8859
Epoch 12: val_loss improved from 4.08587 to 4.08379, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.8861 - val_loss: 4.0838
Epoch 13/250
126/128 [============================>.] - ETA: 0s - loss: 3.8454
Epoch 13: val_loss improved from 4.08379 to 4.07697, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.8434 - val_loss: 4.0770
Epoch 14/250
126/128 [============================>.] - ETA: 0s - loss: 3.8107
Epoch 14: val_loss improved from 4.07697 to 4.04691, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.8121 - val_loss: 4.0469
Epoch 15/250
126/128 [============================>.] - ETA: 0s - loss: 3.7657
Epoch 15: val_loss improved from 4.04691 to 4.03305, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.7681 - val_loss: 4.0330
Epoch 16/250
126/128 [============================>.] - ETA: 0s - loss: 3.7342
Epoch 16: val_loss improved from 4.03305 to 4.02688, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.7340 - val_loss: 4.0269
Epoch 17/250
126/128 [============================>.] - ETA: 0s - loss: 3.7047
Epoch 17: val_loss improved from 4.02688 to 4.01249, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.7040 - val_loss: 4.0125
Epoch 18/250
126/128 [============================>.] - ETA: 0s - loss: 3.6598
Epoch 18: val_loss improved from 4.01249 to 3.99218, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.6603 - val_loss: 3.9922
Epoch 19/250
126/128 [============================>.] - ETA: 0s - loss: 3.6252
Epoch 19: val_loss did not improve from 3.99218
128/128 [==============================] - 2s 13ms/step - loss: 3.6254 - val_loss: 3.9996
Epoch 20/250
126/128 [============================>.] - ETA: 0s - loss: 3.6078
Epoch 20: val_loss improved from 3.99218 to 3.97337, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.6063 - val_loss: 3.9734
Epoch 21/250
125/128 [============================>.] - ETA: 0s - loss: 3.5767
Epoch 21: val_loss did not improve from 3.97337
128/128 [==============================] - 2s 13ms/step - loss: 3.5773 - val_loss: 3.9830
Epoch 22/250
126/128 [============================>.] - ETA: 0s - loss: 3.5410
Epoch 22: val_loss improved from 3.97337 to 3.95829, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.5419 - val_loss: 3.9583
Epoch 23/250
126/128 [============================>.] - ETA: 0s - loss: 3.5155
Epoch 23: val_loss improved from 3.95829 to 3.95560, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.5154 - val_loss: 3.9556
Epoch 24/250
126/128 [============================>.] - ETA: 0s - loss: 3.4984
Epoch 24: val_loss improved from 3.95560 to 3.92432, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.4958 - val_loss: 3.9243
Epoch 25/250
126/128 [============================>.] - ETA: 0s - loss: 3.4639
Epoch 25: val_loss did not improve from 3.92432
128/128 [==============================] - 2s 13ms/step - loss: 3.4650 - val_loss: 3.9369
Epoch 26/250
125/128 [============================>.] - ETA: 0s - loss: 3.4346
Epoch 26: val_loss improved from 3.92432 to 3.92310, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.4326 - val_loss: 3.9231
Epoch 27/250
126/128 [============================>.] - ETA: 0s - loss: 3.4077
Epoch 27: val_loss improved from 3.92310 to 3.90362, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.4082 - val_loss: 3.9036
Epoch 28/250
126/128 [============================>.] - ETA: 0s - loss: 3.3838
Epoch 28: val_loss improved from 3.90362 to 3.90350, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.3823 - val_loss: 3.9035
Epoch 29/250
126/128 [============================>.] - ETA: 0s - loss: 3.3605
Epoch 29: val_loss did not improve from 3.90350
128/128 [==============================] - 2s 13ms/step - loss: 3.3610 - val_loss: 3.9180
Epoch 30/250
126/128 [============================>.] - ETA: 0s - loss: 3.3354
Epoch 30: val_loss improved from 3.90350 to 3.88731, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.3341 - val_loss: 3.8873
Epoch 31/250
126/128 [============================>.] - ETA: 0s - loss: 3.3113
Epoch 31: val_loss improved from 3.88731 to 3.87085, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.3114 - val_loss: 3.8708
Epoch 32/250
126/128 [============================>.] - ETA: 0s - loss: 3.2817
Epoch 32: val_loss did not improve from 3.87085
128/128 [==============================] - 2s 13ms/step - loss: 3.2818 - val_loss: 3.8710
Epoch 33/250
126/128 [============================>.] - ETA: 0s - loss: 3.2638
Epoch 33: val_loss improved from 3.87085 to 3.86970, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.2635 - val_loss: 3.8697
Epoch 34/250
126/128 [============================>.] - ETA: 0s - loss: 3.2448
Epoch 34: val_loss improved from 3.86970 to 3.85745, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.2450 - val_loss: 3.8574
Epoch 35/250
126/128 [============================>.] - ETA: 0s - loss: 3.2295
Epoch 35: val_loss did not improve from 3.85745
128/128 [==============================] - 2s 13ms/step - loss: 3.2320 - val_loss: 3.8692
Epoch 36/250
125/128 [============================>.] - ETA: 0s - loss: 3.1938
Epoch 36: val_loss did not improve from 3.85745
128/128 [==============================] - 2s 13ms/step - loss: 3.1957 - val_loss: 3.8648
Epoch 37/250
126/128 [============================>.] - ETA: 0s - loss: 3.1703
Epoch 37: val_loss did not improve from 3.85745
128/128 [==============================] - 2s 13ms/step - loss: 3.1720 - val_loss: 3.8607
Epoch 38/250
126/128 [============================>.] - ETA: 0s - loss: 3.1554
Epoch 38: val_loss improved from 3.85745 to 3.84518, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.1564 - val_loss: 3.8452
Epoch 39/250
124/128 [============================>.] - ETA: 0s - loss: 3.1441
Epoch 39: val_loss improved from 3.84518 to 3.82960, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 15ms/step - loss: 3.1462 - val_loss: 3.8296
Epoch 40/250
126/128 [============================>.] - ETA: 0s - loss: 3.1117
Epoch 40: val_loss improved from 3.82960 to 3.82192, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.1112 - val_loss: 3.8219
Epoch 41/250
126/128 [============================>.] - ETA: 0s - loss: 3.1012
Epoch 41: val_loss did not improve from 3.82192
128/128 [==============================] - 2s 13ms/step - loss: 3.1023 - val_loss: 3.8280
Epoch 42/250
126/128 [============================>.] - ETA: 0s - loss: 3.0887
Epoch 42: val_loss improved from 3.82192 to 3.81559, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.0916 - val_loss: 3.8156
Epoch 43/250
126/128 [============================>.] - ETA: 0s - loss: 3.0702
Epoch 43: val_loss improved from 3.81559 to 3.81321, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.0718 - val_loss: 3.8132
Epoch 44/250
126/128 [============================>.] - ETA: 0s - loss: 3.0433
Epoch 44: val_loss did not improve from 3.81321
128/128 [==============================] - 2s 13ms/step - loss: 3.0425 - val_loss: 3.8207
Epoch 45/250
126/128 [============================>.] - ETA: 0s - loss: 3.0483
Epoch 45: val_loss improved from 3.81321 to 3.80937, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.0477 - val_loss: 3.8094
Epoch 46/250
126/128 [============================>.] - ETA: 0s - loss: 3.0272
Epoch 46: val_loss improved from 3.80937 to 3.80903, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.0282 - val_loss: 3.8090
Epoch 47/250
126/128 [============================>.] - ETA: 0s - loss: 3.0152
Epoch 47: val_loss improved from 3.80903 to 3.79484, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 3.0130 - val_loss: 3.7948
Epoch 48/250
126/128 [============================>.] - ETA: 0s - loss: 2.9963
Epoch 48: val_loss did not improve from 3.79484
128/128 [==============================] - 2s 13ms/step - loss: 2.9964 - val_loss: 3.8011
Epoch 49/250
126/128 [============================>.] - ETA: 0s - loss: 2.9710
Epoch 49: val_loss did not improve from 3.79484
128/128 [==============================] - 2s 13ms/step - loss: 2.9750 - val_loss: 3.7957
Epoch 50/250
126/128 [============================>.] - ETA: 0s - loss: 2.9644
Epoch 50: val_loss did not improve from 3.79484
128/128 [==============================] - 2s 13ms/step - loss: 2.9648 - val_loss: 3.7959
Epoch 51/250
126/128 [============================>.] - ETA: 0s - loss: 2.9475
Epoch 51: val_loss improved from 3.79484 to 3.78661, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 2.9470 - val_loss: 3.7866
Epoch 52/250
126/128 [============================>.] - ETA: 0s - loss: 2.9493
Epoch 52: val_loss did not improve from 3.78661
128/128 [==============================] - 2s 13ms/step - loss: 2.9496 - val_loss: 3.7974
Epoch 53/250
126/128 [============================>.] - ETA: 0s - loss: 2.9206
Epoch 53: val_loss improved from 3.78661 to 3.78263, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 2.9223 - val_loss: 3.7826
Epoch 54/250
126/128 [============================>.] - ETA: 0s - loss: 2.9155
Epoch 54: val_loss did not improve from 3.78263
128/128 [==============================] - 2s 13ms/step - loss: 2.9155 - val_loss: 3.7967
Epoch 55/250
126/128 [============================>.] - ETA: 0s - loss: 2.9040
Epoch 55: val_loss did not improve from 3.78263
128/128 [==============================] - 2s 13ms/step - loss: 2.9042 - val_loss: 3.7913
Epoch 56/250
125/128 [============================>.] - ETA: 0s - loss: 2.8867
Epoch 56: val_loss did not improve from 3.78263
128/128 [==============================] - 2s 13ms/step - loss: 2.8883 - val_loss: 3.7952
Epoch 57/250
126/128 [============================>.] - ETA: 0s - loss: 2.8728
Epoch 57: val_loss did not improve from 3.78263
128/128 [==============================] - 2s 13ms/step - loss: 2.8720 - val_loss: 3.8102
Epoch 58/250
126/128 [============================>.] - ETA: 0s - loss: 2.8493
Epoch 58: val_loss did not improve from 3.78263
128/128 [==============================] - 2s 13ms/step - loss: 2.8517 - val_loss: 3.7940
Epoch 59/250
126/128 [============================>.] - ETA: 0s - loss: 2.8510
Epoch 59: val_loss did not improve from 3.78263
128/128 [==============================] - 2s 13ms/step - loss: 2.8530 - val_loss: 3.7903
Epoch 60/250
126/128 [============================>.] - ETA: 0s - loss: 2.8394
Epoch 60: val_loss did not improve from 3.78263
128/128 [==============================] - 2s 13ms/step - loss: 2.8410 - val_loss: 3.8045
Epoch 61/250
126/128 [============================>.] - ETA: 0s - loss: 2.8392
Epoch 61: val_loss did not improve from 3.78263
128/128 [==============================] - 2s 13ms/step - loss: 2.8397 - val_loss: 3.7882
Epoch 62/250
126/128 [============================>.] - ETA: 0s - loss: 2.7978
Epoch 62: val_loss improved from 3.78263 to 3.78033, saving model to best_model-wavenet.h5
128/128 [==============================] - 2s 14ms/step - loss: 2.7994 - val_loss: 3.7803
Epoch 63/250
125/128 [============================>.] - ETA: 0s - loss: 2.7912
Epoch 63: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7927 - val_loss: 3.7885
Epoch 64/250
126/128 [============================>.] - ETA: 0s - loss: 2.7873
Epoch 64: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7878 - val_loss: 3.8008
Epoch 65/250
126/128 [============================>.] - ETA: 0s - loss: 2.7865
Epoch 65: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7846 - val_loss: 3.7919
Epoch 66/250
126/128 [============================>.] - ETA: 0s - loss: 2.7713
Epoch 66: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7748 - val_loss: 3.8048
Epoch 67/250
126/128 [============================>.] - ETA: 0s - loss: 2.7735
Epoch 67: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7750 - val_loss: 3.7936
Epoch 68/250
126/128 [============================>.] - ETA: 0s - loss: 2.7600
Epoch 68: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7625 - val_loss: 3.7943
Epoch 69/250
126/128 [============================>.] - ETA: 0s - loss: 2.7352
Epoch 69: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7384 - val_loss: 3.8019
Epoch 70/250
126/128 [============================>.] - ETA: 0s - loss: 2.7341
Epoch 70: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7383 - val_loss: 3.8026
Epoch 71/250
126/128 [============================>.] - ETA: 0s - loss: 2.7236
Epoch 71: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7245 - val_loss: 3.7948
Epoch 72/250
126/128 [============================>.] - ETA: 0s - loss: 2.7076
Epoch 72: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7090 - val_loss: 3.7965
Epoch 73/250
125/128 [============================>.] - ETA: 0s - loss: 2.7064
Epoch 73: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.7095 - val_loss: 3.8043
Epoch 74/250
124/128 [============================>.] - ETA: 0s - loss: 2.7029
Epoch 74: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 15ms/step - loss: 2.7048 - val_loss: 3.8034
Epoch 75/250
126/128 [============================>.] - ETA: 0s - loss: 2.6925
Epoch 75: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6928 - val_loss: 3.8169
Epoch 76/250
126/128 [============================>.] - ETA: 0s - loss: 2.6874
Epoch 76: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6887 - val_loss: 3.8024
Epoch 77/250
126/128 [============================>.] - ETA: 0s - loss: 2.6749
Epoch 77: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6736 - val_loss: 3.8032
Epoch 78/250
126/128 [============================>.] - ETA: 0s - loss: 2.6704
Epoch 78: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6708 - val_loss: 3.8201
Epoch 79/250
126/128 [============================>.] - ETA: 0s - loss: 2.6628
Epoch 79: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6637 - val_loss: 3.8202
Epoch 80/250
126/128 [============================>.] - ETA: 0s - loss: 2.6506
Epoch 80: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6526 - val_loss: 3.8258
Epoch 81/250
125/128 [============================>.] - ETA: 0s - loss: 2.6413
Epoch 81: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6409 - val_loss: 3.8339
Epoch 82/250
126/128 [============================>.] - ETA: 0s - loss: 2.6457
Epoch 82: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6465 - val_loss: 3.8357
Epoch 83/250
126/128 [============================>.] - ETA: 0s - loss: 2.6283
Epoch 83: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6275 - val_loss: 3.8330
Epoch 84/250
125/128 [============================>.] - ETA: 0s - loss: 2.6071
Epoch 84: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6101 - val_loss: 3.8621
Epoch 85/250
126/128 [============================>.] - ETA: 0s - loss: 2.5966
Epoch 85: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5983 - val_loss: 3.8619
Epoch 86/250
126/128 [============================>.] - ETA: 0s - loss: 2.6055
Epoch 86: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.6038 - val_loss: 3.8241
Epoch 87/250
126/128 [============================>.] - ETA: 0s - loss: 2.5939
Epoch 87: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5936 - val_loss: 3.8326
Epoch 88/250
126/128 [============================>.] - ETA: 0s - loss: 2.5823
Epoch 88: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5870 - val_loss: 3.8383
Epoch 89/250
126/128 [============================>.] - ETA: 0s - loss: 2.5784
Epoch 89: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5760 - val_loss: 3.8575
Epoch 90/250
126/128 [============================>.] - ETA: 0s - loss: 2.5808
Epoch 90: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5812 - val_loss: 3.8409
Epoch 91/250
126/128 [============================>.] - ETA: 0s - loss: 2.5641
Epoch 91: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5653 - val_loss: 3.8499
Epoch 92/250
126/128 [============================>.] - ETA: 0s - loss: 2.5688
Epoch 92: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5669 - val_loss: 3.8616
Epoch 93/250
126/128 [============================>.] - ETA: 0s - loss: 2.5616
Epoch 93: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5607 - val_loss: 3.8496
Epoch 94/250
126/128 [============================>.] - ETA: 0s - loss: 2.5492
Epoch 94: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5527 - val_loss: 3.8539
Epoch 95/250
126/128 [============================>.] - ETA: 0s - loss: 2.5420
Epoch 95: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5436 - val_loss: 3.8584
Epoch 96/250
126/128 [============================>.] - ETA: 0s - loss: 2.5380
Epoch 96: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5407 - val_loss: 3.8595
Epoch 97/250
126/128 [============================>.] - ETA: 0s - loss: 2.5250
Epoch 97: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5259 - val_loss: 3.8536
Epoch 98/250
126/128 [============================>.] - ETA: 0s - loss: 2.5220
Epoch 98: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5243 - val_loss: 3.8914
Epoch 99/250
126/128 [============================>.] - ETA: 0s - loss: 2.5368
Epoch 99: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5368 - val_loss: 3.8750
Epoch 100/250
126/128 [============================>.] - ETA: 0s - loss: 2.5199
Epoch 100: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5186 - val_loss: 3.8569
Epoch 101/250
126/128 [============================>.] - ETA: 0s - loss: 2.4997
Epoch 101: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5011 - val_loss: 3.8627
Epoch 102/250
125/128 [============================>.] - ETA: 0s - loss: 2.5026
Epoch 102: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5021 - val_loss: 3.8884
Epoch 103/250
126/128 [============================>.] - ETA: 0s - loss: 2.5056
Epoch 103: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5066 - val_loss: 3.8785
Epoch 104/250
126/128 [============================>.] - ETA: 0s - loss: 2.5120
Epoch 104: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.5139 - val_loss: 3.8737
Epoch 105/250
126/128 [============================>.] - ETA: 0s - loss: 2.4946
Epoch 105: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4960 - val_loss: 3.8656
Epoch 106/250
126/128 [============================>.] - ETA: 0s - loss: 2.4922
Epoch 106: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4920 - val_loss: 3.8624
Epoch 107/250
126/128 [============================>.] - ETA: 0s - loss: 2.4853
Epoch 107: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4838 - val_loss: 3.8720
Epoch 108/250
125/128 [============================>.] - ETA: 0s - loss: 2.4793
Epoch 108: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4810 - val_loss: 3.8984
Epoch 109/250
126/128 [============================>.] - ETA: 0s - loss: 2.4860
Epoch 109: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 15ms/step - loss: 2.4860 - val_loss: 3.8831
Epoch 110/250
125/128 [============================>.] - ETA: 0s - loss: 2.4625
Epoch 110: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4645 - val_loss: 3.9228
Epoch 111/250
125/128 [============================>.] - ETA: 0s - loss: 2.4506
Epoch 111: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4530 - val_loss: 3.8899
Epoch 112/250
126/128 [============================>.] - ETA: 0s - loss: 2.4601
Epoch 112: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4600 - val_loss: 3.8968
Epoch 113/250
126/128 [============================>.] - ETA: 0s - loss: 2.4447
Epoch 113: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4468 - val_loss: 3.8812
Epoch 114/250
126/128 [============================>.] - ETA: 0s - loss: 2.4526
Epoch 114: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4524 - val_loss: 3.9076
Epoch 115/250
126/128 [============================>.] - ETA: 0s - loss: 2.4523
Epoch 115: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4520 - val_loss: 3.8965
Epoch 116/250
125/128 [============================>.] - ETA: 0s - loss: 2.4401
Epoch 116: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4412 - val_loss: 3.9086
Epoch 117/250
126/128 [============================>.] - ETA: 0s - loss: 2.4329
Epoch 117: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4354 - val_loss: 3.9095
Epoch 118/250
126/128 [============================>.] - ETA: 0s - loss: 2.4278
Epoch 118: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4290 - val_loss: 3.8982
Epoch 119/250
126/128 [============================>.] - ETA: 0s - loss: 2.4280
Epoch 119: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4255 - val_loss: 3.9184
Epoch 120/250
126/128 [============================>.] - ETA: 0s - loss: 2.4185
Epoch 120: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4201 - val_loss: 3.9054
Epoch 121/250
126/128 [============================>.] - ETA: 0s - loss: 2.3982
Epoch 121: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4003 - val_loss: 3.9310
Epoch 122/250
126/128 [============================>.] - ETA: 0s - loss: 2.4147
Epoch 122: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4135 - val_loss: 3.9227
Epoch 123/250
125/128 [============================>.] - ETA: 0s - loss: 2.4149
Epoch 123: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4152 - val_loss: 3.9091
Epoch 124/250
126/128 [============================>.] - ETA: 0s - loss: 2.4216
Epoch 124: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4218 - val_loss: 3.9097
Epoch 125/250
126/128 [============================>.] - ETA: 0s - loss: 2.4014
Epoch 125: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.4065 - val_loss: 3.9132
Epoch 126/250
126/128 [============================>.] - ETA: 0s - loss: 2.3761
Epoch 126: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3780 - val_loss: 3.9509
Epoch 127/250
126/128 [============================>.] - ETA: 0s - loss: 2.3913
Epoch 127: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3916 - val_loss: 3.9481
Epoch 128/250
126/128 [============================>.] - ETA: 0s - loss: 2.3887
Epoch 128: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3901 - val_loss: 3.9476
Epoch 129/250
126/128 [============================>.] - ETA: 0s - loss: 2.3875
Epoch 129: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3861 - val_loss: 3.9514
Epoch 130/250
126/128 [============================>.] - ETA: 0s - loss: 2.3782
Epoch 130: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3794 - val_loss: 3.9294
Epoch 131/250
126/128 [============================>.] - ETA: 0s - loss: 2.3746
Epoch 131: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3747 - val_loss: 3.9466
Epoch 132/250
126/128 [============================>.] - ETA: 0s - loss: 2.3489
Epoch 132: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3532 - val_loss: 3.9557
Epoch 133/250
126/128 [============================>.] - ETA: 0s - loss: 2.3609
Epoch 133: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3598 - val_loss: 3.9605
Epoch 134/250
126/128 [============================>.] - ETA: 0s - loss: 2.3702
Epoch 134: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3687 - val_loss: 3.9307
Epoch 135/250
126/128 [============================>.] - ETA: 0s - loss: 2.3487
Epoch 135: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3499 - val_loss: 3.9445
Epoch 136/250
125/128 [============================>.] - ETA: 0s - loss: 2.3584
Epoch 136: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3569 - val_loss: 3.9774
Epoch 137/250
126/128 [============================>.] - ETA: 0s - loss: 2.3534
Epoch 137: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3552 - val_loss: 3.9548
Epoch 138/250
125/128 [============================>.] - ETA: 0s - loss: 2.3283
Epoch 138: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3313 - val_loss: 3.9724
Epoch 139/250
126/128 [============================>.] - ETA: 0s - loss: 2.3417
Epoch 139: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3412 - val_loss: 3.9689
Epoch 140/250
125/128 [============================>.] - ETA: 0s - loss: 2.3227
Epoch 140: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3217 - val_loss: 3.9907
Epoch 141/250
126/128 [============================>.] - ETA: 0s - loss: 2.3429
Epoch 141: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3395 - val_loss: 3.9788
Epoch 142/250
126/128 [============================>.] - ETA: 0s - loss: 2.3349
Epoch 142: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3339 - val_loss: 3.9826
Epoch 143/250
126/128 [============================>.] - ETA: 0s - loss: 2.3149
Epoch 143: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3174 - val_loss: 4.0110
Epoch 144/250
126/128 [============================>.] - ETA: 0s - loss: 2.3214
Epoch 144: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 15ms/step - loss: 2.3212 - val_loss: 3.9740
Epoch 145/250
125/128 [============================>.] - ETA: 0s - loss: 2.2949
Epoch 145: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 14ms/step - loss: 2.2968 - val_loss: 4.0058
Epoch 146/250
126/128 [============================>.] - ETA: 0s - loss: 2.3281
Epoch 146: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3294 - val_loss: 3.9995
Epoch 147/250
126/128 [============================>.] - ETA: 0s - loss: 2.3231
Epoch 147: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3239 - val_loss: 3.9989
Epoch 148/250
126/128 [============================>.] - ETA: 0s - loss: 2.3177
Epoch 148: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3198 - val_loss: 4.0150
Epoch 149/250
126/128 [============================>.] - ETA: 0s - loss: 2.3148
Epoch 149: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3170 - val_loss: 4.0064
Epoch 150/250
126/128 [============================>.] - ETA: 0s - loss: 2.2916
Epoch 150: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2914 - val_loss: 3.9997
Epoch 151/250
126/128 [============================>.] - ETA: 0s - loss: 2.3092
Epoch 151: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3080 - val_loss: 4.0169
Epoch 152/250
126/128 [============================>.] - ETA: 0s - loss: 2.3072
Epoch 152: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3063 - val_loss: 4.0149
Epoch 153/250
126/128 [============================>.] - ETA: 0s - loss: 2.2962
Epoch 153: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2982 - val_loss: 4.0088
Epoch 154/250
126/128 [============================>.] - ETA: 0s - loss: 2.2996
Epoch 154: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.3004 - val_loss: 4.0131
Epoch 155/250
126/128 [============================>.] - ETA: 0s - loss: 2.2898
Epoch 155: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2887 - val_loss: 4.0252
Epoch 156/250
126/128 [============================>.] - ETA: 0s - loss: 2.2739
Epoch 156: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2736 - val_loss: 4.0301
Epoch 157/250
126/128 [============================>.] - ETA: 0s - loss: 2.2808
Epoch 157: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2820 - val_loss: 4.0300
Epoch 158/250
126/128 [============================>.] - ETA: 0s - loss: 2.2757
Epoch 158: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2753 - val_loss: 4.0456
Epoch 159/250
126/128 [============================>.] - ETA: 0s - loss: 2.2788
Epoch 159: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2815 - val_loss: 4.0338
Epoch 160/250
126/128 [============================>.] - ETA: 0s - loss: 2.2715
Epoch 160: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2720 - val_loss: 4.0143
Epoch 161/250
126/128 [============================>.] - ETA: 0s - loss: 2.2670
Epoch 161: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2647 - val_loss: 4.0301
Epoch 162/250
126/128 [============================>.] - ETA: 0s - loss: 2.2626
Epoch 162: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2632 - val_loss: 4.0091
Epoch 163/250
125/128 [============================>.] - ETA: 0s - loss: 2.2562
Epoch 163: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2596 - val_loss: 4.0318
Epoch 164/250
126/128 [============================>.] - ETA: 0s - loss: 2.2606
Epoch 164: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2589 - val_loss: 4.0108
Epoch 165/250
126/128 [============================>.] - ETA: 0s - loss: 2.2487
Epoch 165: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2523 - val_loss: 4.0116
Epoch 166/250
126/128 [============================>.] - ETA: 0s - loss: 2.2448
Epoch 166: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2443 - val_loss: 4.0277
Epoch 167/250
126/128 [============================>.] - ETA: 0s - loss: 2.2690
Epoch 167: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2691 - val_loss: 4.0413
Epoch 168/250
126/128 [============================>.] - ETA: 0s - loss: 2.2374
Epoch 168: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2376 - val_loss: 4.0702
Epoch 169/250
126/128 [============================>.] - ETA: 0s - loss: 2.2433
Epoch 169: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2432 - val_loss: 4.0380
Epoch 170/250
126/128 [============================>.] - ETA: 0s - loss: 2.2424
Epoch 170: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2440 - val_loss: 4.0361
Epoch 171/250
126/128 [============================>.] - ETA: 0s - loss: 2.2434
Epoch 171: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2412 - val_loss: 4.0335
Epoch 172/250
126/128 [============================>.] - ETA: 0s - loss: 2.2429
Epoch 172: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2436 - val_loss: 4.0576
Epoch 173/250
126/128 [============================>.] - ETA: 0s - loss: 2.2269
Epoch 173: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2279 - val_loss: 4.0358
Epoch 174/250
126/128 [============================>.] - ETA: 0s - loss: 2.2381
Epoch 174: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2374 - val_loss: 4.0659
Epoch 175/250
126/128 [============================>.] - ETA: 0s - loss: 2.2272
Epoch 175: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2269 - val_loss: 4.0565
Epoch 176/250
126/128 [============================>.] - ETA: 0s - loss: 2.2164
Epoch 176: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2172 - val_loss: 4.0597
Epoch 177/250
126/128 [============================>.] - ETA: 0s - loss: 2.2230
Epoch 177: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2228 - val_loss: 4.0678
Epoch 178/250
126/128 [============================>.] - ETA: 0s - loss: 2.2366
Epoch 178: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2368 - val_loss: 4.0605
Epoch 179/250
125/128 [============================>.] - ETA: 0s - loss: 2.2280
Epoch 179: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2307 - val_loss: 4.0539
Epoch 180/250
124/128 [============================>.] - ETA: 0s - loss: 2.2103
Epoch 180: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 14ms/step - loss: 2.2149 - val_loss: 4.0929
Epoch 181/250
126/128 [============================>.] - ETA: 0s - loss: 2.2024
Epoch 181: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2049 - val_loss: 4.0813
Epoch 182/250
126/128 [============================>.] - ETA: 0s - loss: 2.2223
Epoch 182: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2224 - val_loss: 4.0791
Epoch 183/250
126/128 [============================>.] - ETA: 0s - loss: 2.2079
Epoch 183: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2106 - val_loss: 4.0706
Epoch 184/250
126/128 [============================>.] - ETA: 0s - loss: 2.1882
Epoch 184: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1881 - val_loss: 4.0706
Epoch 185/250
126/128 [============================>.] - ETA: 0s - loss: 2.1971
Epoch 185: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1974 - val_loss: 4.1159
Epoch 186/250
126/128 [============================>.] - ETA: 0s - loss: 2.2013
Epoch 186: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2012 - val_loss: 4.0776
Epoch 187/250
126/128 [============================>.] - ETA: 0s - loss: 2.2058
Epoch 187: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2072 - val_loss: 4.0731
Epoch 188/250
126/128 [============================>.] - ETA: 0s - loss: 2.1904
Epoch 188: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1898 - val_loss: 4.0862
Epoch 189/250
126/128 [============================>.] - ETA: 0s - loss: 2.1957
Epoch 189: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1981 - val_loss: 4.0946
Epoch 190/250
126/128 [============================>.] - ETA: 0s - loss: 2.2020
Epoch 190: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.2007 - val_loss: 4.0702
Epoch 191/250
126/128 [============================>.] - ETA: 0s - loss: 2.1697
Epoch 191: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1733 - val_loss: 4.0872
Epoch 192/250
126/128 [============================>.] - ETA: 0s - loss: 2.1896
Epoch 192: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1903 - val_loss: 4.1058
Epoch 193/250
126/128 [============================>.] - ETA: 0s - loss: 2.1793
Epoch 193: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1799 - val_loss: 4.1029
Epoch 194/250
126/128 [============================>.] - ETA: 0s - loss: 2.1745
Epoch 194: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1752 - val_loss: 4.1118
Epoch 195/250
126/128 [============================>.] - ETA: 0s - loss: 2.1736
Epoch 195: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1743 - val_loss: 4.1072
Epoch 196/250
126/128 [============================>.] - ETA: 0s - loss: 2.1832
Epoch 196: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1849 - val_loss: 4.1121
Epoch 197/250
126/128 [============================>.] - ETA: 0s - loss: 2.1631
Epoch 197: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1608 - val_loss: 4.1066
Epoch 198/250
126/128 [============================>.] - ETA: 0s - loss: 2.1635
Epoch 198: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1661 - val_loss: 4.1261
Epoch 199/250
126/128 [============================>.] - ETA: 0s - loss: 2.1567
Epoch 199: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1580 - val_loss: 4.1590
Epoch 200/250
126/128 [============================>.] - ETA: 0s - loss: 2.1780
Epoch 200: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1779 - val_loss: 4.1264
Epoch 201/250
126/128 [============================>.] - ETA: 0s - loss: 2.1489
Epoch 201: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1512 - val_loss: 4.1309
Epoch 202/250
125/128 [============================>.] - ETA: 0s - loss: 2.1681
Epoch 202: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1691 - val_loss: 4.1116
Epoch 203/250
126/128 [============================>.] - ETA: 0s - loss: 2.1532
Epoch 203: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1548 - val_loss: 4.1088
Epoch 204/250
126/128 [============================>.] - ETA: 0s - loss: 2.1621
Epoch 204: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1632 - val_loss: 4.0826
Epoch 205/250
126/128 [============================>.] - ETA: 0s - loss: 2.1693
Epoch 205: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1683 - val_loss: 4.0716
Epoch 206/250
126/128 [============================>.] - ETA: 0s - loss: 2.1589
Epoch 206: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1632 - val_loss: 4.1100
Epoch 207/250
126/128 [============================>.] - ETA: 0s - loss: 2.1491
Epoch 207: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1506 - val_loss: 4.1430
Epoch 208/250
126/128 [============================>.] - ETA: 0s - loss: 2.1381
Epoch 208: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1372 - val_loss: 4.1227
Epoch 209/250
126/128 [============================>.] - ETA: 0s - loss: 2.1314
Epoch 209: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1316 - val_loss: 4.1602
Epoch 210/250
126/128 [============================>.] - ETA: 0s - loss: 2.1404
Epoch 210: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1401 - val_loss: 4.1494
Epoch 211/250
126/128 [============================>.] - ETA: 0s - loss: 2.1316
Epoch 211: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1333 - val_loss: 4.1656
Epoch 212/250
126/128 [============================>.] - ETA: 0s - loss: 2.1411
Epoch 212: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1427 - val_loss: 4.1477
Epoch 213/250
126/128 [============================>.] - ETA: 0s - loss: 2.1312
Epoch 213: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1317 - val_loss: 4.1587
Epoch 214/250
126/128 [============================>.] - ETA: 0s - loss: 2.1402
Epoch 214: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1379 - val_loss: 4.1364
Epoch 215/250
126/128 [============================>.] - ETA: 0s - loss: 2.1334
Epoch 215: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1357 - val_loss: 4.1574
Epoch 216/250
125/128 [============================>.] - ETA: 0s - loss: 2.1356
Epoch 216: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 14ms/step - loss: 2.1375 - val_loss: 4.1279
Epoch 217/250
126/128 [============================>.] - ETA: 0s - loss: 2.1273
Epoch 217: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1312 - val_loss: 4.1199
Epoch 218/250
126/128 [============================>.] - ETA: 0s - loss: 2.1249
Epoch 218: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1276 - val_loss: 4.1443
Epoch 219/250
126/128 [============================>.] - ETA: 0s - loss: 2.1218
Epoch 219: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1217 - val_loss: 4.1382
Epoch 220/250
126/128 [============================>.] - ETA: 0s - loss: 2.1343
Epoch 220: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1342 - val_loss: 4.1536
Epoch 221/250
126/128 [============================>.] - ETA: 0s - loss: 2.1045
Epoch 221: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1048 - val_loss: 4.1560
Epoch 222/250
126/128 [============================>.] - ETA: 0s - loss: 2.1185
Epoch 222: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1191 - val_loss: 4.1499
Epoch 223/250
126/128 [============================>.] - ETA: 0s - loss: 2.0989
Epoch 223: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1006 - val_loss: 4.1717
Epoch 224/250
126/128 [============================>.] - ETA: 0s - loss: 2.1065
Epoch 224: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1068 - val_loss: 4.1475
Epoch 225/250
126/128 [============================>.] - ETA: 0s - loss: 2.1114
Epoch 225: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1100 - val_loss: 4.1640
Epoch 226/250
126/128 [============================>.] - ETA: 0s - loss: 2.1107
Epoch 226: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1123 - val_loss: 4.1878
Epoch 227/250
126/128 [============================>.] - ETA: 0s - loss: 2.1200
Epoch 227: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1203 - val_loss: 4.1727
Epoch 228/250
126/128 [============================>.] - ETA: 0s - loss: 2.1102
Epoch 228: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1103 - val_loss: 4.1609
Epoch 229/250
126/128 [============================>.] - ETA: 0s - loss: 2.0979
Epoch 229: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0972 - val_loss: 4.1713
Epoch 230/250
126/128 [============================>.] - ETA: 0s - loss: 2.1002
Epoch 230: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0982 - val_loss: 4.2035
Epoch 231/250
126/128 [============================>.] - ETA: 0s - loss: 2.0979
Epoch 231: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0969 - val_loss: 4.1591
Epoch 232/250
125/128 [============================>.] - ETA: 0s - loss: 2.0924
Epoch 232: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0970 - val_loss: 4.1605
Epoch 233/250
125/128 [============================>.] - ETA: 0s - loss: 2.0937
Epoch 233: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0954 - val_loss: 4.1729
Epoch 234/250
126/128 [============================>.] - ETA: 0s - loss: 2.1037
Epoch 234: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1023 - val_loss: 4.2076
Epoch 235/250
126/128 [============================>.] - ETA: 0s - loss: 2.1087
Epoch 235: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.1085 - val_loss: 4.2016
Epoch 236/250
126/128 [============================>.] - ETA: 0s - loss: 2.0925
Epoch 236: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0924 - val_loss: 4.1528
Epoch 237/250
126/128 [============================>.] - ETA: 0s - loss: 2.0902
Epoch 237: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0899 - val_loss: 4.1947
Epoch 238/250
126/128 [============================>.] - ETA: 0s - loss: 2.0886
Epoch 238: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0880 - val_loss: 4.1628
Epoch 239/250
126/128 [============================>.] - ETA: 0s - loss: 2.0869
Epoch 239: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0872 - val_loss: 4.1776
Epoch 240/250
126/128 [============================>.] - ETA: 0s - loss: 2.0975
Epoch 240: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0975 - val_loss: 4.1894
Epoch 241/250
126/128 [============================>.] - ETA: 0s - loss: 2.0984
Epoch 241: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0983 - val_loss: 4.1961
Epoch 242/250
126/128 [============================>.] - ETA: 0s - loss: 2.0803
Epoch 242: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0841 - val_loss: 4.1500
Epoch 243/250
126/128 [============================>.] - ETA: 0s - loss: 2.0744
Epoch 243: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0761 - val_loss: 4.1804
Epoch 244/250
125/128 [============================>.] - ETA: 0s - loss: 2.0604
Epoch 244: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0632 - val_loss: 4.1970
Epoch 245/250
126/128 [============================>.] - ETA: 0s - loss: 2.0763
Epoch 245: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0777 - val_loss: 4.1838
Epoch 246/250
126/128 [============================>.] - ETA: 0s - loss: 2.0854
Epoch 246: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0873 - val_loss: 4.1990
Epoch 247/250
126/128 [============================>.] - ETA: 0s - loss: 2.0863
Epoch 247: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0879 - val_loss: 4.1777
Epoch 248/250
126/128 [============================>.] - ETA: 0s - loss: 2.0785
Epoch 248: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0804 - val_loss: 4.1978
Epoch 249/250
126/128 [============================>.] - ETA: 0s - loss: 2.0584
Epoch 249: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0591 - val_loss: 4.2048
Epoch 250/250
126/128 [============================>.] - ETA: 0s - loss: 2.0655
Epoch 250: val_loss did not improve from 3.78033
128/128 [==============================] - 2s 13ms/step - loss: 2.0667 - val_loss: 4.2153
loss_list: [4.60108757019043, 4.453006744384766, 4.375421047210693, 4.34819221496582, 4.303104877471924, 4.270804405212402, 4.230975151062012, 4.2015180587768555, 4.152528285980225, 4.132441997528076, 4.085867881774902, 4.08378791809082, 4.076972007751465, 4.046914577484131, 4.033045768737793, 4.026882648468018, 4.012488842010498, 3.9921817779541016, 3.9996073246002197, 3.9733738899230957, 3.982962131500244, 3.958292007446289, 3.955604314804077, 3.9243216514587402, 3.936906337738037, 3.923100471496582, 3.903616189956665, 3.903502941131592, 3.9179632663726807, 3.887312650680542, 3.870849132537842, 3.8709938526153564, 3.8696982860565186, 3.857447624206543, 3.8692424297332764, 3.8647890090942383, 3.8606889247894287, 3.8451826572418213, 3.8295960426330566, 3.821920156478882, 3.8279881477355957, 3.8155903816223145, 3.813213586807251, 3.8207435607910156, 3.8093719482421875, 3.8090288639068604, 3.794844150543213, 3.801112413406372, 3.795701265335083, 3.7958567142486572, 3.786609411239624, 3.797353744506836, 3.7826292514801025, 3.7966792583465576, 3.791255235671997, 3.795243501663208, 3.810192823410034, 3.7940027713775635, 3.7903029918670654, 3.8045194149017334, 3.7881581783294678, 3.7803258895874023, 3.788451910018921, 3.8007569313049316, 3.7919020652770996, 3.804840087890625, 3.7935760021209717, 3.794325828552246, 3.8019297122955322, 3.80263352394104, 3.7947709560394287, 3.7965099811553955, 3.8042521476745605, 3.803373098373413, 3.816924810409546, 3.8023886680603027, 3.8032405376434326, 3.8201303482055664, 3.820220947265625, 3.8258042335510254, 3.833857297897339, 3.8357391357421875, 3.832986831665039, 3.862107276916504, 3.8619470596313477, 3.824120283126831, 3.8325767517089844, 3.8382811546325684, 3.8574929237365723, 3.840930700302124, 3.8499181270599365, 3.861595869064331, 3.849602460861206, 3.8538522720336914, 3.8584372997283936, 3.8595147132873535, 3.8536150455474854, 3.891386032104492, 3.874969005584717, 3.856919050216675, 3.862675428390503, 3.8884150981903076, 3.8785266876220703, 3.873664379119873, 3.8656280040740967, 3.8624258041381836, 3.8719897270202637, 3.8984487056732178, 3.8830690383911133, 3.9227778911590576, 3.8898744583129883, 3.896758556365967, 3.881195545196533, 3.9075560569763184, 3.896491050720215, 3.908569097518921, 3.909477949142456, 3.898163080215454, 3.918351888656616, 3.9053592681884766, 3.9309887886047363, 3.9227118492126465, 3.9091362953186035, 3.909661293029785, 3.9132156372070312, 3.9508843421936035, 3.9481234550476074, 3.9475560188293457, 3.951415777206421, 3.9293782711029053, 3.9465813636779785, 3.9557178020477295, 3.9604716300964355, 3.930734634399414, 3.944486618041992, 3.977435827255249, 3.954754590988159, 3.9724199771881104, 3.968888998031616, 3.990654230117798, 3.978781223297119, 3.9825828075408936, 4.010988712310791, 3.973975896835327, 4.005794048309326, 3.99954891204834, 3.9989471435546875, 4.014950752258301, 4.006424427032471, 3.999725103378296, 4.016865253448486, 4.014917850494385, 4.008841514587402, 4.013138771057129, 4.025186061859131, 4.030110836029053, 4.029958248138428, 4.045559883117676, 4.033848762512207, 4.014308929443359, 4.03010368347168, 4.009142875671387, 4.0318450927734375, 4.0108489990234375, 4.0116190910339355, 4.027679443359375, 4.0412702560424805, 4.07020902633667, 4.038045883178711, 4.036093235015869, 4.033482074737549, 4.057642936706543, 4.035815238952637, 4.065923690795898, 4.056464195251465, 4.059703826904297, 4.067753791809082, 4.060492992401123, 4.053862571716309, 4.092861652374268, 4.081275463104248, 4.079139232635498, 4.070596218109131, 4.07056188583374, 4.115922927856445, 4.077588081359863, 4.073112487792969, 4.0861992835998535, 4.0946221351623535, 4.070216178894043, 4.087162017822266, 4.105807781219482, 4.1029276847839355, 4.1118268966674805, 4.107210159301758, 4.112080097198486, 4.1065874099731445, 4.126080513000488, 4.159032344818115, 4.126410484313965, 4.130880355834961, 4.1116414070129395, 4.108846664428711, 4.082637310028076, 4.071645736694336, 4.109976768493652, 4.142961025238037, 4.1227126121521, 4.160186290740967, 4.149449825286865, 4.165628433227539, 4.1477251052856445, 4.158715724945068, 4.136375904083252, 4.157444477081299, 4.127936363220215, 4.119913101196289, 4.144318580627441, 4.1381659507751465, 4.153557300567627, 4.1560187339782715, 4.149862766265869, 4.171738147735596, 4.147476673126221, 4.16402530670166, 4.187756061553955, 4.1726908683776855, 4.16090726852417, 4.17134952545166, 4.2035417556762695, 4.159114360809326, 4.160495281219482, 4.172947883605957, 4.20756721496582, 4.201592445373535, 4.152785778045654, 4.1947021484375, 4.162751197814941, 4.177649974822998, 4.189440727233887, 4.196097373962402, 4.150032997131348, 4.180353164672852, 4.196977615356445, 4.183816909790039, 4.199036121368408, 4.177670478820801, 4.197781562805176, 4.20478630065918, 4.215262413024902]
name: best_model-wavenet.h5
random: 154
random_music: [  8  53  92  32  91 114  33 116  86  91  38  32 117  11  38  54  38  54
  45  71  93  31  21  31 125  50  27 109  93   5 103   8]
random music size: (32,)
[65, 47, 65, 65, 65, 23, 65, 23, 65, 23, 65, 23, 23, 23, 23, 23, 23, 23, 23, 23, 83, 65, 83, 83, 90, 90, 90, 83, 83, 90, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 94, 83, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 24, 24, 24, 24, 24, 24, 24, 24, 24, 53, 53, 53, 53, 31, 31, 81, 31, 81, 81, 81, 104, 31, 81, 50, 50, 1, 1, 103, 47, 47, 52, 52, 9, 47, 9, 47, 9, 47, 52, 57, 84, 81, 123, 57, 24, 50, 24, 24, 24, 12, 119, 12, 50, 24, 50, 50, 81, 77, 38, 24, 50, 77, 38, 77, 55, 24, 55, 24, 55, 24, 97, 24, 24, 10, 10, 24, 83, 83, 83, 83, 83, 83, 49, 123, 81, 20, 20, 20, 118, 66, 118, 66, 66, 52, 47, 52, 47, 52, 24, 47, 124, 24, 124, 47, 124, 124, 110, 57, 57, 57, 57, 112, 112, 105, 104, 104, 104, 104, 104, 0, 0, 74, 81, 0, 74, 74, 47, 103, 47, 23, 34, 65, 47, 23, 23, 23, 47, 23, 47, 2, 34, 15, 2, 2, 2, 2, 23, 7, 23, 23, 23, 23, 23, 83, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 58, 58, 58, 58, 58, 58, 100, 100, 100, 100, 100, 100, 100, 100, 100, 80, 80, 80, 92, 92, 92, 92, 41, 41, 41, 93, 41, 41, 41, 41, 123, 123, 12, 123, 47, 84, 83, 47, 47, 47, 47, 123, 123, 21, 47, 47, 47, 47, 47, 47, 47, 47, 47, 57, 15, 47, 47, 47, 47, 57, 47, 3, 3, 3, 3, 3, 63, 63, 63, 63, 63, 63, 47, 63, 57, 63, 81, 81, 81, 57, 57, 57, 57, 57, 81, 57, 57, 57, 57, 57, 57, 57, 81, 57, 81, 57, 81, 26, 57, 81, 81, 81, 81, 81, 18, 57, 8, 18, 8, 125, 18, 18, 18, 18, 18, 102, 102, 18, 124, 81, 124, 81, 102, 57, 124, 57, 57, 57, 57, 57, 57, 57, 57, 57, 81, 57, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 74, 81, 81, 102, 102, 102, 102, 102, 18, 18, 18, 102, 57, 18, 57, 102, 102, 102, 57, 57, 102, 57, 74, 102, 18, 18, 18, 57, 57, 57, 57, 57, 74, 74, 74, 74, 26, 114, 26, 81, 26, 57, 81, 74, 90, 74, 74, 74, 74, 63, 63, 26, 124, 26, 26, 81, 20, 20, 20, 20, 63, 63, 20, 102, 102, 102, 18, 102, 124, 6, 6, 39, 18, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102]
['1.6', '9.1', '1.6', '1.6', '1.6', 'E3', '1.6', 'E3', '1.6', 'E3', '1.6', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'B1', '1.6', 'B1', 'B1', '9', '9', '9', 'B1', 'B1', '9', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', '5.9.0', 'B1', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', 'C3', '10', '10', '10', '10', '10', '10', '10', '10', '10', 'C#6', 'C#6', 'C#6', 'C#6', '10.1', '10.1', '1', '10.1', '1', '1', '1', 'A6', '10.1', '1', 'G3', 'G3', '6', '6', '6.11', '9.1', '9.1', 'C2', 'C2', 'F3', '9.1', 'F3', '9.1', 'F3', '9.1', 'C2', '3', '2.5', '1', '7.0', '3', '10', 'G3', '10', '10', '10', '8.0', '0.5', '8.0', 'G3', '10', 'G3', 'G3', '1', '0', 'F5', '10', 'G3', '0', 'F5', '0', 'F#1', '10', 'F#1', '10', 'F#1', '10', '2', '10', '10', 'E-2', 'E-2', '10', 'B1', 'B1', 'B1', 'B1', 'B1', 'B1', 'G#6', '7.0', '1', '11', '11', '11', '2.4', 'B-2', '2.4', 'B-2', 'B-2', 'C2', '9.1', 'C2', '9.1', 'C2', '10', '9.1', '8', '10', '8', '9.1', '8', '8', 'E-3', '3', '3', '3', '3', 'F#6', 'F#6', '6.9', 'A6', 'A6', 'A6', 'A6', 'A6', '4.7', '4.7', '9.2', '1', '4.7', '9.2', '9.2', '9.1', '6.11', '9.1', 'E3', 'A4', '1.6', '9.1', 'E3', 'E3', 'E3', '9.1', 'E3', '9.1', 'F#4', 'A4', 'D4', 'F#4', 'F#4', 'F#4', 'F#4', 'E3', '4.9', 'E3', 'E3', 'E3', 'E3', 'E3', 'B1', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', 'F#6', '4.10', '4.10', '4.10', '4.10', '4.10', '4.10', '4.8', '4.8', '4.8', '4.8', '4.8', '4.8', '4.8', '4.8', '4.8', '10.3', '10.3', '10.3', 'B-5', 'B-5', 'B-5', 'B-5', '7.10', '7.10', '7.10', '10.2', '7.10', '7.10', '7.10', '7.10', '7.0', '7.0', '8.0', '7.0', '9.1', '2.5', 'B1', '9.1', '9.1', '9.1', '9.1', '7.0', '7.0', '11.3', '9.1', '9.1', '9.1', '9.1', '9.1', '9.1', '9.1', '9.1', '9.1', '3', 'D4', '9.1', '9.1', '9.1', '9.1', '3', '9.1', '3.8', '3.8', '3.8', '3.8', '3.8', 'D2', 'D2', 'D2', 'D2', 'D2', 'D2', '9.1', 'D2', '3', 'D2', '1', '1', '1', '3', '3', '3', '3', '3', '1', '3', '3', '3', '3', '3', '3', '3', '1', '3', '1', '3', '1', 'C#2', '3', '1', '1', '1', '1', '1', '7', '3', 'B-3', '7', 'B-3', 'G#3', '7', '7', '7', '7', '7', '4', '4', '7', '8', '1', '8', '1', '4', '3', '8', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '9.2', '1', '1', '4', '4', '4', '4', '4', '7', '7', '7', '4', '3', '7', '3', '4', '4', '4', '3', '3', '4', '3', '9.2', '4', '7', '7', '7', '3', '3', '3', '3', '3', '9.2', '9.2', '9.2', '9.2', 'C#2', 'C#5', 'C#2', '1', 'C#2', '3', '1', '9.2', '9', '9.2', '9.2', '9.2', '9.2', 'D2', 'D2', 'C#2', '8', 'C#2', 'C#2', '1', '11', '11', '11', '11', 'D2', 'D2', '11', '4', '4', '4', '7', '4', '8', '6.8', '6.8', 'A1', '7', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4']