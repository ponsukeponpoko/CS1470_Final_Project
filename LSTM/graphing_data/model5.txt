Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 32, 200)           25200     
_________________________________________________________________
lstm (LSTM)                  (None, 32, 256)           467968    
_________________________________________________________________
lstm_1 (LSTM)                (None, 32, 256)           525312    
_________________________________________________________________
lstm_2 (LSTM)                (None, 256)               525312    
_________________________________________________________________
dense (Dense)                (None, 512)               131584    
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 126)               64638     
_________________________________________________________________
activation_1 (Activation)    (None, 126)               0         
=================================================================
Total params: 1,740,014
Trainable params: 1,740,014
Non-trainable params: 0
_________________________________________________________________
unique x: 126
unique y: 126
2022-05-10 15:17:37.166448: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/250
128/128 [==============================] - 47s 329ms/step - loss: 4.4965 - val_loss: 4.3568

Epoch 00001: val_loss improved from inf to 4.35684, saving model to best_model-lstm.h5
Epoch 2/250
128/128 [==============================] - 46s 357ms/step - loss: 4.3132 - val_loss: 4.2441

Epoch 00002: val_loss improved from 4.35684 to 4.24412, saving model to best_model-lstm.h5
Epoch 3/250
128/128 [==============================] - 45s 351ms/step - loss: 4.1807 - val_loss: 4.1462

Epoch 00003: val_loss improved from 4.24412 to 4.14620, saving model to best_model-lstm.h5
Epoch 4/250
128/128 [==============================] - 44s 343ms/step - loss: 4.0857 - val_loss: 4.1138

Epoch 00004: val_loss improved from 4.14620 to 4.11377, saving model to best_model-lstm.h5
Epoch 5/250
128/128 [==============================] - 44s 341ms/step - loss: 4.0072 - val_loss: 4.0691

Epoch 00005: val_loss improved from 4.11377 to 4.06912, saving model to best_model-lstm.h5
Epoch 6/250
128/128 [==============================] - 54s 419ms/step - loss: 3.9185 - val_loss: 4.0397

Epoch 00006: val_loss improved from 4.06912 to 4.03970, saving model to best_model-lstm.h5
Epoch 7/250
128/128 [==============================] - 50s 392ms/step - loss: 3.8379 - val_loss: 4.0023

Epoch 00007: val_loss improved from 4.03970 to 4.00226, saving model to best_model-lstm.h5
Epoch 8/250
128/128 [==============================] - 52s 409ms/step - loss: 3.7232 - val_loss: 3.9868

Epoch 00008: val_loss improved from 4.00226 to 3.98680, saving model to best_model-lstm.h5
Epoch 9/250
128/128 [==============================] - 50s 387ms/step - loss: 3.6140 - val_loss: 3.9624

Epoch 00009: val_loss improved from 3.98680 to 3.96237, saving model to best_model-lstm.h5
Epoch 10/250
128/128 [==============================] - 51s 402ms/step - loss: 3.4722 - val_loss: 3.9511

Epoch 00010: val_loss improved from 3.96237 to 3.95108, saving model to best_model-lstm.h5
Epoch 11/250
128/128 [==============================] - 48s 373ms/step - loss: 3.3135 - val_loss: 3.9393

Epoch 00011: val_loss improved from 3.95108 to 3.93929, saving model to best_model-lstm.h5
Epoch 12/250
128/128 [==============================] - 44s 347ms/step - loss: 3.1304 - val_loss: 3.9490

Epoch 00012: val_loss did not improve from 3.93929
Epoch 13/250
128/128 [==============================] - 46s 357ms/step - loss: 2.9262 - val_loss: 3.9694

Epoch 00013: val_loss did not improve from 3.93929
Epoch 14/250
128/128 [==============================] - 44s 346ms/step - loss: 2.7241 - val_loss: 4.0500

Epoch 00014: val_loss did not improve from 3.93929
Epoch 15/250
128/128 [==============================] - 45s 352ms/step - loss: 2.5136 - val_loss: 4.0866

Epoch 00015: val_loss did not improve from 3.93929
Epoch 16/250
128/128 [==============================] - 45s 350ms/step - loss: 2.2962 - val_loss: 4.2026

Epoch 00016: val_loss did not improve from 3.93929
Epoch 17/250
128/128 [==============================] - 44s 347ms/step - loss: 2.0662 - val_loss: 4.3269

Epoch 00017: val_loss did not improve from 3.93929
Epoch 18/250
128/128 [==============================] - 46s 362ms/step - loss: 1.8337 - val_loss: 4.4794

Epoch 00018: val_loss did not improve from 3.93929
Epoch 19/250
128/128 [==============================] - 45s 353ms/step - loss: 1.6153 - val_loss: 4.6387

Epoch 00019: val_loss did not improve from 3.93929
Epoch 20/250
128/128 [==============================] - 45s 353ms/step - loss: 1.3924 - val_loss: 4.8728

Epoch 00020: val_loss did not improve from 3.93929
Epoch 21/250
128/128 [==============================] - 51s 402ms/step - loss: 1.1801 - val_loss: 5.1062

Epoch 00021: val_loss did not improve from 3.93929
Epoch 22/250
128/128 [==============================] - 51s 401ms/step - loss: 0.9695 - val_loss: 5.3577

Epoch 00022: val_loss did not improve from 3.93929
Epoch 23/250
 10/128 [=>............................] - ETA: 39s - loss: 0.6848^CTraceback (most recent call last):
  File "LSTM/experiment_2.py", line 352, in <module>
    callbacks=[mc],